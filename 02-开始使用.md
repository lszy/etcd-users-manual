# 系统要求 #
低延迟的内存和硬盘，高性能的CPU，官方测试环境以供参考的配置为 CPU:8vCPUs，RAM:16G，SSD:50G。
# 安装 #
## 预编译安装包 ##
## 使用容器镜像 ##
## 编译 ##
### 要求 ###
* Go （版本>1.8）
* Git （可选）
### 编译安装 ###
```
Shell># git clone https://github.com/coreos/etcd.git
Shell># cd etcd
Shell># ./build
```
### 启动 ###
我们可以如下命令启动etcd
```
Shell># etcd
```
### 验证 ###
设置一个key
```
ETCDCTL_API=3 ./bin/etcdctl put foo bar
```
etcd会返回如下信息
```
OK
```
## 关于etcd命令 ##
### 用法  ###
```
Shell># etcd -- version 			显示软件版本
Shell># etcd -h | --help			打印帮助信息
Shell># etcd --config-file		指定配置文件
Shell># etcd [flags]				启动一个etcd服务
```

#### 标志（flags）####

标志|功能
----|----
--name 'default'|友好节点名
--data-dir '${name}.etcd'|数据文件存储路径
--wal-dir ''|Wal路径
--snapshot-count '10000'|保存到硬盘提交的数据数量
--heartbeat-interval '100'|心跳时间（ms）
--election-timeout '1000'|
--listen-peer-urls 'http://localhost:2380'|list of URLs to listen on for peer traffic.
--listen-client-urls 'http://localhost:2379'|list of URLs to listen on for client traffic.
--max-snapshots '5'|maximum number of snapshot files to retain (0 is unlimited).
--max-wals '5'|maximum number of wal files to retain (0 is unlimited).
--cors ''|comma-separated whitelist of origins for CORS (cross-origin resource sharing).
--quota-backend-bytes '0'|raise alarms when backend size exceeds the given quota (0 defaults to low space quota)

#### 群集（clustering flags） ####
标志|功能
----|----
--initial-advertise-peer-urls 'http://localhost:2380'|list of this member's peer URLs to advertise to the rest of the cluster.
--initial-cluster 'default=http://localhost:2380'|initial cluster configuration for bootstrapping.
--initial-cluster-state 'new'|initial cluster state ('new' or 'existing').
--initial-cluster-token 'etcd-cluster'|initial cluster token for the etcd cluster during bootstrap.Specifying this can protect you from unintended cross-cluster interaction when running multiple clusters.
--advertise-client-urls 'http://localhost:2379'|list of this member's client URLs to advertise to the public.The client URLs advertised should be accessible to machines that talk to etcd cluster. etcd client libraries parse these URLs to connect to the cluster.
--discovery ''|discovery URL used to bootstrap the cluster.
--discovery-fallback 'proxy'|expected behavior ('exit' or 'proxy') when discovery services fails.
"proxy" supports v2 API only.
--discovery-proxy ''|HTTP proxy to use for traffic to discovery service.
--discovery-srv ''|dns srv domain used to bootstrap the cluster.
--strict-reconfig-check|reject reconfiguration requests that would cause quorum loss.
--auto-compaction-retention '0'|auto compaction retention in hour. 0 means disable auto compaction.

#### 代理（proxy flags） ####
* 仅支持v2 APi

标志|功能
----|----
--proxy 'off'|proxy mode setting ('off', 'readonly' or 'on').
--proxy-failure-wait 5000|time (ms) an endpoint will be held in a failed state.
--proxy-refresh-interval 30000|time (in milliseconds) of the endpoints refresh interval.
--proxy-dial-timeout 1000|time (in milliseconds) for a dial to timeout.
--proxy-write-timeout 5000|time (in milliseconds) for a write to timeout.
--proxy-read-timeout 0|time (in milliseconds) for a read to timeout.

#### 安全（security flags） ####
标志|功能
----|----
--ca-file '' [DEPRECATED]|path to the client server TLS CA file. '-ca-file ca.crt' could be replaced by '-trusted-ca-file ca.crt -client-cert-auth' and etcd will perform the same.
--cert-file ''|path to the client server TLS cert file.
--key-file ''|path to the client server TLS key file.
--client-cert-auth 'false'|enable client cert authentication.
--trusted-ca-file ''|path to the client server TLS trusted CA key file.
--auto-tls 'false'|client TLS using generated certificates.
--peer-ca-file '' [DEPRECATED]|path to the peer server TLS CA file. '-peer-ca-file ca.crt' could be replaced by '-peer-trusted-ca-file ca.crt -peer-client-cert-auth' and etcd will perform the same.
--peer-cert-file ''|path to the peer server TLS cert file.
--peer-key-file ''|path to the peer server TLS key file.
--peer-client-cert-auth 'false'|enable peer client cert authentication.
--peer-trusted-ca-file ''|path to the peer server TLS trusted CA file.
--peer-auto-tls 'false'|peer TLS using self-generated certificates if --peer-key-file and --peer-cert-file are not provided.

#### 日志（logging flags） ####
标志|功能
----|----
--debug 'false'|enable debug-level logging for etcd.
--log-package-levels ''|specify a particular log level for each etcd package (eg: 'etcdmain=CRITICAL,etcdserver=DEBUG').
--log-output 'default'|specify 'stdout' or 'stderr' to skip journald logging even when running under systemd.

#### 不安全（unsafe flags） ####
* Please be CAUTIOUS when using unsafe flags because it will break the guarantees given by the consensus protocol.

标志|功能
----|----
--force-new-cluster 'false'|force to create a new one-member cluster.


#### 策略（profiling flags） ####
标志|功能
----|----
--enable-pprof 'false'|Enable runtime profiling data via HTTP server. Address is at client URL + "/debug/pprof/"
--metrics 'basic'|Set level of detail for exported metrics, specify 'extensive' to include histogram metrics.

# 调整 #
Etcd的默认设置在本地低延迟网络中运行良好，但是，当我们使用etcd穿越数据中心时，就会出现高延迟。那么心跳时间和选举超时时间就必须调整。
还有就是磁盘较慢的时候，也需要调整。
## 时间参数 ##
The underlying distributed consensus protocol relies on two separate time parameters to ensure that nodes can handoff leadership if one stalls or goes offline. The first parameter is called the Heartbeat Interval. This is the frequency with which the leader will notify followers that it is still the leader. For best practices, the parameter should be set around round-trip time between members. By default, etcd uses a 100ms heartbeat interval.

The second parameter is the Election Timeout. This timeout is how long a follower node will go without hearing a heartbeat before attempting to become leader itself. By default, etcd uses a 1000ms election timeout.

Adjusting these values is a trade off. The value of heartbeat interval is recommended to be around the maximum of average round-trip time (RTT) between members, normally around 0.5-1.5x the round-trip time. If heartbeat interval is too low, etcd will send unnecessary messages that increase the usage of CPU and network resources. On the other side, a too high heartbeat interval leads to high election timeout. Higher election timeout takes longer time to detect a leader failure. The easiest way to measure round-trip time (RTT) is to use PING utility.

The election timeout should be set based on the heartbeat interval and average round-trip time between members. Election timeouts must be at least 10 times the round-trip time so it can account for variance in the network. For example, if the round-trip time between members is 10ms then the election timeout should be at least 100ms.

The upper limit of election timeout is 50000ms (50s), which should only be used when deploying a globally-distributed etcd cluster. A reasonable round-trip time for the continental United States is 130ms, and the time between US and Japan is around 350-400ms. If the network has uneven performance or regular packet delays/loss then it is possible that a couple of retries may be necessary to successfully send a packet. So 5s is a safe upper limit of global round-trip time. As the election timeout should be an order of magnitude bigger than broadcast time, in the case of ~5s for a globally distributed cluster, then 50 seconds becomes a reasonable maximum.

The heartbeat interval and election timeout value should be the same for all members in one cluster. Setting different values for etcd members may disrupt cluster stability.
使用如下命令调整
```
Shell># etcd --hearbeat-interval=100 --election-time=500
```
也可以使用环境变量
```
Shell># ETCD_HEARTBEAT_INTERVAL=100 ETCD_ELECTION_TIMEOUT=500 etcd
```
* 上面的单位是毫秒
## 快照 ##
etcd 追加全部key的改变到日志文件中。 This log grows forever and is a complete linear history of every change made to the keys. A complete history works well for lightly used clusters but clusters that are heavily used would carry around a large log.

To avoid having a huge log etcd makes periodic snapshots. These snapshots provide a way for etcd to compact the log by saving the current state of the system and removing old logs.

Snapshot tuning
Creating snapshots with the V2 backend can be expensive, so snapshots are only created after a given number of changes to etcd. By default, snapshots will be made after every 10,000 changes. If etcd's memory usage and disk usage are too high, try lowering the snapshot threshold by setting the following on the command line:
可以使用如下命令
```
Shell># etcd --snapshot-count=5000
```
也可以使用环境变量
```
Shell>#ETCD_SNAPSHOT_COUNT=5000 etcd
```
## 硬盘 ##
An etcd cluster is very sensitive to disk latencies. Since etcd must persist proposals to its log, disk activity from other processes may cause long fsync latencies. The upshot is etcd may miss heartbeats, causing request timeouts and temporary leader loss. An etcd server can sometimes stably run alongside these processes when given a high disk priority.

On Linux, etcd's disk priority can be configured with ionice:
```
Shell># ionice -c2 -n0 -p `pgrep etcd`
```

## 网络 ##
If the etcd leader serves a large number of concurrent client requests, it may delay processing follower peer requests due to network congestion. This manifests as send buffer error messages on the follower nodes:
dropped MsgProp to 247ae21ff9436b2d since streamMsg's sending buffer is full
dropped MsgAppResp to 247ae21ff9436b2d since streamMsg's sending buffer is full
These errors may be resolved by prioritizing etcd's peer traffic over its client traffic. On Linux, peer traffic can be prioritized by using the traffic control mechanism:
```
Shell># tc qdisc add dev eth0 root handle 1: prio bands 3
Shell># tc filter add dev eth0 parent 1: protocol ip prio 1 u32 match ip sport 2380 0xffff flowid 1:1
Shell># tc filter add dev eth0 parent 1: protocol ip prio 1 u32 match ip dport 2380 0xffff flowid 1:1
Shell># tc filter add dev eth0 parent 1: protocol ip prio 2 u32 match ip sport 2739 0xffff flowid 1:1
Shell># tc filter add dev eth0 parent 1: protocol ip prio 2 u32 match ip dport 2739 0xffff flowid 1:1
```

# 压力测试 #
压力测试可以通过benchmark 完成。
基准环境
* GCE 
* 服务端：3台  8vCPUs , 16G RAM , 50G SSD
* 客户端：1台 16vCPUs , 30G RAM , 50G SSD
* OS : Ubuntu 17.04
* Etcd 3.2.0 go 1.8.3
### 写入性能 ###
Key数量|Key大小|值大小|连接数|客户端|目标服务器|平均QPS|平均写入时间|平均RSS
----|---|---|---|---|---|---|---|----
10000|8BYTES|256BYTES|1|1|Leader|583|1.6ms|48MB
100000|8BYTES|256BYTES|100|1000|Leader|44341|22ms|124MB
100000|8BYTES|256BYTES|100|1000|All|50104|20ms|126MB
命令
```
写入到leader
benchmark --endpoints=${HOST_1} --target-leader --conns=1 --clients=1 \
    put --key-size=8 --sequential-keys --total=10000 --val-size=256
benchmark --endpoints=${HOST_1} --target-leader  --conns=100 --clients=1000 \
put --key-size=8 --sequential-keys --total=100000 --val-size=256
写入到全部
benchmark --endpoints=${HOST_1},${HOST_2},${HOST_3} --conns=100 --clients=1000 \
    put --key-size=8 --sequential-keys --total=100000 --val-size=256
```
### 读取性能 ###
Key数量|key大小|值大小|连接数|客户端|读取密度|平均QPS|平均读取时间
----|---|---|---|---|---|---|---|----
10000|8BYTES|256BYTES|1|1|Linearizable|1353|0.7ms
10000|8BYTES|256BYTES|1|1|Serializable|2909|0.3ms
100000|8BYTES|256BYTES|100|1000|Linearizable|141578|5.5ms
100000|8BYTES|256BYTES|100|1000|Serializable|185758|2.2ms
命令
```
一个连接
benchmark --endpoints=${HOST_1},${HOST_2},${HOST_3} --conns=1 --clients=1 \
    range YOUR_KEY --consistency=l --total=10000
benchmark --endpoints=${HOST_1},${HOST_2},${HOST_3} --conns=1 --clients=1 \
range YOUR_KEY --consistency=s --total=10000
多个连接
benchmark --endpoints=${HOST_1},${HOST_2},${HOST_3} --conns=100 --clients=1000 \
    range YOUR_KEY --consistency=l --total=100000
benchmark --endpoints=${HOST_1},${HOST_2},${HOST_3} --conns=100 --clients=1000 \
    range YOUR_KEY --consistency=s --total=100000
```

